{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79de7756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da33c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 10000, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_len, d_model)  # (L, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, L, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, L, d_model)\n",
    "        x = x + self.pe[:, : x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "def causal_mask(L: int, device=None):\n",
    "    # shape: (L, L), True where masked\n",
    "    mask = torch.triu(torch.ones(L, L, device=device, dtype=torch.bool), diagonal=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int,\n",
    "        d_model: int = 128,\n",
    "        nhead: int = 8,\n",
    "        num_layers: int = 4,\n",
    "        dim_feedforward: int = 256,\n",
    "        dropout: float = 0.1,\n",
    "        out_dim: int = 1,  # = num_classes для классификации\n",
    "        problem_type: str = \"regression\",  # or \"classification\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.problem_type = problem_type\n",
    "        self.input_proj = nn.Linear(n_features, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, dropout=dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        # x: (B, L, n_features)\n",
    "        B, L, _ = x.size()\n",
    "        h = self.input_proj(x)\n",
    "        h = self.pos_enc(h)\n",
    "        mask = causal_mask(L, device=x.device)  # (L, L) True=masked\n",
    "        h = self.encoder(h, mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        logits = self.head(h)  # (B, L, out_dim)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fb16964",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/frames_errors.csv\", header=None)\n",
    "df.columns = [\n",
    "    \"block_id\",\n",
    "    \"frame_idx\",\n",
    "    \"E_mu_Z\",\n",
    "    \"E_mu_phys_est\",\n",
    "    \"E_mu_X\",\n",
    "    \"E_nu1_X\",\n",
    "    \"E_nu2_X\",\n",
    "    \"E_nu1_Z\",\n",
    "    \"E_nu2_Z\",\n",
    "    \"N_mu_X\",\n",
    "    \"M_mu_XX\",\n",
    "    \"M_mu_XZ\",\n",
    "    \"M_mu_X\",\n",
    "    \"N_mu_Z\",\n",
    "    \"M_mu_ZZ\",\n",
    "    \"M_mu_Z\",\n",
    "    \"N_nu1_X\",\n",
    "    \"M_nu1_XX\",\n",
    "    \"M_nu1_XZ\",\n",
    "    \"M_nu1_X\",\n",
    "    \"N_nu1_Z\",\n",
    "    \"M_nu1_ZZ\",\n",
    "    \"M_nu1_Z\",\n",
    "    \"N_nu2_X\",\n",
    "    \"M_nu2_XX\",\n",
    "    \"M_nu2_XZ\",\n",
    "    \"M_nu2_X\",\n",
    "    \"N_nu2_Z\",\n",
    "    \"M_nu2_ZZ\",\n",
    "    \"M_nu2_Z\",\n",
    "    \"nTot\",\n",
    "    \"bayesImVoltage\",\n",
    "    \"opticalPower\",\n",
    "    \"polarizerVoltages[0]\",\n",
    "    \"polarizerVoltages[1]\",\n",
    "    \"polarizerVoltages[2]\",\n",
    "    \"polarizerVoltages[3]\",\n",
    "    \"temp_1\",\n",
    "    \"biasVoltage_1\",\n",
    "    \"temp_2\",\n",
    "    \"biasVoltage_2\",\n",
    "    \"synErr\",\n",
    "    \"N_EC_rounds\",\n",
    "    \"maintenance_flag\",\n",
    "    \"estimator_name\",\n",
    "    \"f_EC\",\n",
    "    \"E_mu_Z_est\",\n",
    "    \"R\",\n",
    "    \"s\",\n",
    "    \"p\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3844663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_features = [\n",
    "    \"block_id\",\n",
    "    \"frame_idx\",\n",
    "]\n",
    "\n",
    "phys_gt_features = [\n",
    "    \"E_mu_Z\",\n",
    "    \"E_mu_X\",\n",
    "    \"E_nu1_X\",\n",
    "    \"E_nu2_X\",\n",
    "    \"E_nu1_Z\",\n",
    "    \"E_nu2_Z\",\n",
    "    \"N_mu_X\",\n",
    "    \"M_mu_XX\",\n",
    "    \"M_mu_XZ\",\n",
    "    \"M_mu_X\",\n",
    "    \"N_mu_Z\",\n",
    "    \"M_mu_ZZ\",\n",
    "    \"M_mu_Z\",\n",
    "    \"N_nu1_X\",\n",
    "    \"M_nu1_XX\",\n",
    "    \"M_nu1_XZ\",\n",
    "    \"M_nu1_X\",\n",
    "    \"N_nu1_Z\",\n",
    "    \"M_nu1_ZZ\",\n",
    "    \"M_nu1_Z\",\n",
    "    \"N_nu2_X\",\n",
    "    \"M_nu2_XX\",\n",
    "    \"M_nu2_XZ\",\n",
    "    \"M_nu2_X\",\n",
    "    \"N_nu2_Z\",\n",
    "    \"M_nu2_ZZ\",\n",
    "    \"M_nu2_Z\",\n",
    "]\n",
    "\n",
    "phys_features = [\n",
    "    \"bayesImVoltage\",\n",
    "    \"opticalPower\",\n",
    "    \"polarizerVoltages[0]\",\n",
    "    \"polarizerVoltages[1]\",\n",
    "    \"polarizerVoltages[2]\",\n",
    "    \"polarizerVoltages[3]\",\n",
    "    \"temp_1\",\n",
    "    \"biasVoltage_1\",\n",
    "    \"temp_2\",\n",
    "    \"biasVoltage_2\",\n",
    "]\n",
    "\n",
    "est_features = [\n",
    "    # \"E_mu_phys_est\",\n",
    "    \"E_mu_Z_est\",\n",
    "    \"R\",\n",
    "    \"s\",\n",
    "    \"p\",\n",
    "]\n",
    "\n",
    "proxy_features = [\n",
    "    \"N_EC_rounds\",\n",
    "    # \"f_EC\",\n",
    "]\n",
    "\n",
    "df = df[id_features + phys_gt_features + phys_features + est_features + proxy_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbf4401a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_features(df, features, lag):\n",
    "    df = df.copy()\n",
    "    lagged_features = []\n",
    "\n",
    "    for feature in features:\n",
    "        for l in lag:\n",
    "            df[f\"{feature}_lag_{l}\"] = df.groupby(\"block_id\")[feature].shift(l)\n",
    "            lagged_features.append(f\"{feature}_lag_{l}\")\n",
    "\n",
    "    df = df.bfill().ffill()\n",
    "    return df, lagged_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dad25233",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_create = phys_gt_features + phys_features + est_features\n",
    "df, lagged_features = lag_features(df, features_to_create, lag=[2, 5, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0d41a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 326221\n"
     ]
    }
   ],
   "source": [
    "# Test (Val)\n",
    "start_idx = df[(df[\"block_id\"] == 1489460492) & (df[\"frame_idx\"] == 99)].index[0]\n",
    "end_idx = df[(df[\"block_id\"] == 1840064900) & (df[\"frame_idx\"] == 101)].index[0]\n",
    "\n",
    "test_df = df.loc[start_idx:end_idx].copy()\n",
    "assert len(test_df) == 2000, \"Test (Val) set size is not 2000 rows\"\n",
    "\n",
    "# Train\n",
    "all_block_ids = df[\"block_id\"].unique()\n",
    "train_blocks = [bid for bid in all_block_ids if bid not in test_df[\"block_id\"].values]\n",
    "\n",
    "train_df = df[df[\"block_id\"].isin(train_blocks)]\n",
    "# train_df = train_df[(train_df[\"N_EC_rounds\"] <= 2)]\n",
    "\n",
    "print(f\"Train: {len(train_df['block_id'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85be54a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = phys_features + lagged_features + [\"E_mu_Z_est\", \"R\"]\n",
    "target_col = \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03b06884",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df[feature_cols])\n",
    "\n",
    "train_df.loc[:, feature_cols] = scaler.transform(train_df[feature_cols])\n",
    "test_df.loc[:, feature_cols] = scaler.transform(test_df[feature_cols])\n",
    "\n",
    "train_df.loc[:, feature_cols] = train_df[feature_cols].astype(np.float32)\n",
    "test_df.loc[:, feature_cols] = test_df[feature_cols].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14f33685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seqences(df, feature_cols, target_col):\n",
    "    groups = df.groupby(\"block_id\")\n",
    "    sequences = []\n",
    "    for block_id, g in groups:\n",
    "        x = torch.tensor(g[feature_cols].values, dtype=torch.float32)  # (L_i, F)\n",
    "        y = torch.tensor(g[target_col].values, dtype=torch.float32).unsqueeze(-1)  # (L_i, 1)\n",
    "        sequences.append((x, y))\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd399f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs = get_seqences(train_df, feature_cols, target_col)\n",
    "val_seqs = get_seqences(test_df, feature_cols, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f733cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, seqs):\n",
    "        self.seqs = seqs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.seqs[idx]\n",
    "\n",
    "\n",
    "def collate_pad(batch, pad_value: float = 0.0):\n",
    "    # batch: list of (x_i: (L_i,F), y_i: (L_i,1))\n",
    "    xs, ys = zip(*batch)\n",
    "    lengths = torch.tensor([t.size(0) for t in xs], dtype=torch.long)  # (B,)\n",
    "    X = torch.nn.utils.rnn.pad_sequence(\n",
    "        xs, batch_first=True, padding_value=pad_value\n",
    "    )  # (B, Lmax, F)\n",
    "    Y = torch.nn.utils.rnn.pad_sequence(ys, batch_first=True, padding_value=0.0)  # (B, Lmax, 1)\n",
    "    B, Lmax, _ = X.shape\n",
    "    ar = torch.arange(Lmax).expand(B, Lmax)\n",
    "    pad_mask = ar >= lengths.unsqueeze(1)  # True там, где паддинг: (B, Lmax), bool\n",
    "    loss_mask = (~pad_mask).unsqueeze(-1)  # (B, Lmax, 1) для маскирования лосса\n",
    "    return X, Y, pad_mask, loss_mask, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ec4f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SeqDataset(train_seqs)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_pad)\n",
    "\n",
    "val_dataset = SeqDataset(val_seqs)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cc4820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_features = len(feature_cols)\n",
    "model = TimeSeriesTransformer(\n",
    "    n_features=n_features, out_dim=1, d_model=128, dim_feedforward=256, problem_type=\"regression\"\n",
    ").to(device)\n",
    "loss_fn = nn.MSELoss(reduction=\"none\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92ffccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train RMSE: 3686.801311 | val RMSE: 3720.672215\n",
      "Epoch 02 | train RMSE: 3682.370898 | val RMSE: 3716.537916\n",
      "Epoch 03 | train RMSE: 3677.940811 | val RMSE: 3711.771579\n",
      "Epoch 04 | train RMSE: 3672.899912 | val RMSE: 3706.427127\n",
      "Epoch 05 | train RMSE: 3667.307881 | val RMSE: 3700.568467\n",
      "Epoch 06 | train RMSE: 3661.244665 | val RMSE: 3694.247203\n",
      "Epoch 07 | train RMSE: 3654.721455 | val RMSE: 3687.495115\n",
      "Epoch 08 | train RMSE: 3647.779711 | val RMSE: 3680.329863\n",
      "Epoch 09 | train RMSE: 3640.439623 | val RMSE: 3672.761159\n",
      "Epoch 10 | train RMSE: 3632.692604 | val RMSE: 3664.794464\n",
      "Epoch 11 | train RMSE: 3624.559859 | val RMSE: 3656.433200\n",
      "Epoch 12 | train RMSE: 3616.026006 | val RMSE: 3647.681109\n",
      "Epoch 13 | train RMSE: 3607.110598 | val RMSE: 3638.542970\n",
      "Epoch 14 | train RMSE: 3597.805363 | val RMSE: 3629.023203\n",
      "Epoch 15 | train RMSE: 3588.117205 | val RMSE: 3619.125021\n",
      "Epoch 16 | train RMSE: 3578.061105 | val RMSE: 3608.853256\n",
      "Epoch 17 | train RMSE: 3567.626646 | val RMSE: 3598.210115\n",
      "Epoch 18 | train RMSE: 3556.824607 | val RMSE: 3587.198573\n",
      "Epoch 19 | train RMSE: 3545.635989 | val RMSE: 3575.820667\n",
      "Epoch 20 | train RMSE: 3534.102902 | val RMSE: 3564.078203\n",
      "Epoch 21 | train RMSE: 3522.196021 | val RMSE: 3551.973477\n",
      "Epoch 22 | train RMSE: 3509.918348 | val RMSE: 3539.507690\n",
      "Epoch 23 | train RMSE: 3497.290407 | val RMSE: 3526.684417\n",
      "Epoch 24 | train RMSE: 3484.249451 | val RMSE: 3513.504698\n",
      "Epoch 25 | train RMSE: 3470.876078 | val RMSE: 3499.970944\n",
      "Epoch 26 | train RMSE: 3457.092446 | val RMSE: 3486.074771\n",
      "Epoch 27 | train RMSE: 3442.930166 | val RMSE: 3471.814484\n",
      "Epoch 28 | train RMSE: 3428.400552 | val RMSE: 3457.193868\n",
      "Epoch 29 | train RMSE: 3413.509077 | val RMSE: 3442.269912\n",
      "Epoch 30 | train RMSE: 3398.317690 | val RMSE: 3426.906579\n",
      "Epoch 31 | train RMSE: 3382.694127 | val RMSE: 3411.236923\n",
      "Epoch 32 | train RMSE: 3366.751874 | val RMSE: 3395.208445\n",
      "Epoch 33 | train RMSE: 3350.508423 | val RMSE: 3378.848164\n",
      "Epoch 34 | train RMSE: 3333.854955 | val RMSE: 3362.126108\n",
      "Epoch 35 | train RMSE: 3316.856316 | val RMSE: 3345.055374\n",
      "Epoch 36 | train RMSE: 3299.550604 | val RMSE: 3327.633826\n",
      "Epoch 37 | train RMSE: 3281.891153 | val RMSE: 3309.863814\n",
      "Epoch 38 | train RMSE: 3263.851549 | val RMSE: 3291.757580\n",
      "Epoch 39 | train RMSE: 3245.545971 | val RMSE: 3273.300631\n",
      "Epoch 40 | train RMSE: 3226.833433 | val RMSE: 3254.500372\n",
      "Epoch 41 | train RMSE: 3207.802548 | val RMSE: 3235.358414\n",
      "Epoch 42 | train RMSE: 3188.419700 | val RMSE: 3215.881868\n",
      "Epoch 43 | train RMSE: 3168.727746 | val RMSE: 3196.054688\n",
      "Epoch 44 | train RMSE: 3148.662312 | val RMSE: 3175.910608\n",
      "Epoch 45 | train RMSE: 3128.312995 | val RMSE: 3155.410297\n",
      "Epoch 46 | train RMSE: 3107.536754 | val RMSE: 3134.573063\n",
      "Epoch 47 | train RMSE: 3086.477983 | val RMSE: 3113.408915\n",
      "Epoch 48 | train RMSE: 3065.099844 | val RMSE: 3091.901866\n",
      "Epoch 49 | train RMSE: 3043.319469 | val RMSE: 3070.066058\n",
      "Epoch 50 | train RMSE: 3021.254507 | val RMSE: 3047.898119\n",
      "Epoch 51 | train RMSE: 2998.861875 | val RMSE: 3025.400565\n",
      "Epoch 52 | train RMSE: 2976.119959 | val RMSE: 3002.577581\n",
      "Epoch 53 | train RMSE: 2953.072800 | val RMSE: 2979.427016\n",
      "Epoch 54 | train RMSE: 2929.668125 | val RMSE: 2955.953472\n",
      "Epoch 55 | train RMSE: 2905.973371 | val RMSE: 2932.155652\n",
      "Epoch 56 | train RMSE: 2881.933064 | val RMSE: 2908.045139\n",
      "Epoch 57 | train RMSE: 2857.573838 | val RMSE: 2883.608229\n",
      "Epoch 58 | train RMSE: 2832.889836 | val RMSE: 2858.840671\n",
      "Epoch 59 | train RMSE: 2807.852805 | val RMSE: 2833.763609\n",
      "Epoch 60 | train RMSE: 2782.566563 | val RMSE: 2808.384885\n",
      "Epoch 61 | train RMSE: 2756.925884 | val RMSE: 2782.671131\n",
      "Epoch 62 | train RMSE: 2730.926068 | val RMSE: 2756.641295\n",
      "Epoch 63 | train RMSE: 2704.597461 | val RMSE: 2730.299371\n",
      "Epoch 64 | train RMSE: 2678.017218 | val RMSE: 2703.649113\n",
      "Epoch 65 | train RMSE: 2651.114423 | val RMSE: 2676.690972\n",
      "Epoch 66 | train RMSE: 2623.912379 | val RMSE: 2649.424535\n",
      "Epoch 67 | train RMSE: 2596.383810 | val RMSE: 2621.853756\n",
      "Epoch 68 | train RMSE: 2568.540834 | val RMSE: 2593.979590\n",
      "Epoch 69 | train RMSE: 2540.375684 | val RMSE: 2565.802823\n",
      "Epoch 70 | train RMSE: 2511.874788 | val RMSE: 2537.327501\n",
      "Epoch 71 | train RMSE: 2483.154294 | val RMSE: 2508.554580\n",
      "Epoch 72 | train RMSE: 2454.121242 | val RMSE: 2479.488876\n",
      "Epoch 73 | train RMSE: 2424.784544 | val RMSE: 2450.131987\n",
      "Epoch 74 | train RMSE: 2395.052964 | val RMSE: 2420.474373\n",
      "Epoch 75 | train RMSE: 2365.137281 | val RMSE: 2390.529903\n",
      "Epoch 76 | train RMSE: 2334.910645 | val RMSE: 2360.301432\n",
      "Epoch 77 | train RMSE: 2304.335084 | val RMSE: 2329.784938\n",
      "Epoch 78 | train RMSE: 2273.508679 | val RMSE: 2298.984318\n",
      "Epoch 79 | train RMSE: 2242.421723 | val RMSE: 2267.904591\n",
      "Epoch 80 | train RMSE: 2211.005522 | val RMSE: 2236.547740\n",
      "Epoch 81 | train RMSE: 2179.292935 | val RMSE: 2204.920055\n",
      "Epoch 82 | train RMSE: 2147.356930 | val RMSE: 2173.020784\n",
      "Epoch 83 | train RMSE: 2115.188739 | val RMSE: 2140.852137\n",
      "Epoch 84 | train RMSE: 2082.631201 | val RMSE: 2108.427038\n",
      "Epoch 85 | train RMSE: 2049.810203 | val RMSE: 2075.737540\n",
      "Epoch 86 | train RMSE: 2016.807496 | val RMSE: 2042.791314\n",
      "Epoch 87 | train RMSE: 1983.465857 | val RMSE: 2009.592484\n",
      "Epoch 88 | train RMSE: 1949.947537 | val RMSE: 1976.146262\n",
      "Epoch 89 | train RMSE: 1916.133690 | val RMSE: 1942.462997\n",
      "Epoch 90 | train RMSE: 1882.023009 | val RMSE: 1908.548181\n",
      "Epoch 91 | train RMSE: 1847.776868 | val RMSE: 1874.422296\n",
      "Epoch 92 | train RMSE: 1813.258122 | val RMSE: 1840.060869\n",
      "Epoch 93 | train RMSE: 1778.499539 | val RMSE: 1805.473917\n",
      "Epoch 94 | train RMSE: 1743.538148 | val RMSE: 1770.685526\n",
      "Epoch 95 | train RMSE: 1708.318317 | val RMSE: 1735.684322\n",
      "Epoch 96 | train RMSE: 1672.850954 | val RMSE: 1700.496530\n",
      "Epoch 97 | train RMSE: 1637.154320 | val RMSE: 1665.116856\n",
      "Epoch 98 | train RMSE: 1601.264960 | val RMSE: 1629.554246\n",
      "Epoch 99 | train RMSE: 1565.215057 | val RMSE: 1593.836207\n",
      "Epoch 100 | train RMSE: 1529.071807 | val RMSE: 1557.968482\n",
      "Epoch 101 | train RMSE: 1492.696810 | val RMSE: 1521.967569\n",
      "Epoch 102 | train RMSE: 1456.318817 | val RMSE: 1485.851180\n",
      "Epoch 103 | train RMSE: 1419.674016 | val RMSE: 1449.636158\n",
      "Epoch 104 | train RMSE: 1382.924270 | val RMSE: 1413.337474\n",
      "Epoch 105 | train RMSE: 1346.080522 | val RMSE: 1376.988157\n",
      "Epoch 106 | train RMSE: 1309.136468 | val RMSE: 1340.598075\n",
      "Epoch 107 | train RMSE: 1272.093546 | val RMSE: 1304.199543\n",
      "Epoch 108 | train RMSE: 1235.180338 | val RMSE: 1267.821363\n",
      "Epoch 109 | train RMSE: 1198.170643 | val RMSE: 1231.500574\n",
      "Epoch 110 | train RMSE: 1161.267271 | val RMSE: 1195.266825\n",
      "Epoch 111 | train RMSE: 1124.308545 | val RMSE: 1159.174824\n",
      "Epoch 112 | train RMSE: 1087.657411 | val RMSE: 1123.261480\n",
      "Epoch 113 | train RMSE: 1051.248072 | val RMSE: 1087.592924\n",
      "Epoch 114 | train RMSE: 1015.104882 | val RMSE: 1052.235289\n",
      "Epoch 115 | train RMSE: 978.826924 | val RMSE: 1017.239748\n",
      "Epoch 116 | train RMSE: 943.406755 | val RMSE: 982.703732\n",
      "Epoch 117 | train RMSE: 908.393770 | val RMSE: 948.725393\n",
      "Epoch 118 | train RMSE: 873.754944 | val RMSE: 915.418750\n",
      "Epoch 119 | train RMSE: 839.858762 | val RMSE: 882.895020\n",
      "Epoch 120 | train RMSE: 806.996038 | val RMSE: 851.267140\n",
      "Epoch 121 | train RMSE: 775.176088 | val RMSE: 820.741596\n",
      "Epoch 122 | train RMSE: 744.289401 | val RMSE: 791.445705\n",
      "Epoch 123 | train RMSE: 714.923399 | val RMSE: 763.584473\n",
      "Epoch 124 | train RMSE: 689.002526 | val RMSE: 737.771780\n",
      "Epoch 125 | train RMSE: 661.884114 | val RMSE: 713.741533\n",
      "Epoch 126 | train RMSE: 638.516238 | val RMSE: 691.673009\n",
      "Epoch 127 | train RMSE: 616.793622 | val RMSE: 664.441757\n",
      "Epoch 128 | train RMSE: 563.278056 | val RMSE: 584.946794\n",
      "Epoch 129 | train RMSE: 522.108064 | val RMSE: 554.278255\n",
      "Epoch 130 | train RMSE: 492.301382 | val RMSE: 523.938073\n",
      "Epoch 131 | train RMSE: 464.256904 | val RMSE: 494.364756\n",
      "Epoch 132 | train RMSE: 436.378659 | val RMSE: 465.195494\n",
      "Epoch 133 | train RMSE: 408.367259 | val RMSE: 435.995615\n",
      "Epoch 134 | train RMSE: 382.676948 | val RMSE: 407.731323\n",
      "Epoch 135 | train RMSE: 356.522685 | val RMSE: 379.257780\n",
      "Epoch 136 | train RMSE: 329.593914 | val RMSE: 347.658637\n",
      "Epoch 137 | train RMSE: 303.057295 | val RMSE: 320.378638\n",
      "Epoch 138 | train RMSE: 278.108306 | val RMSE: 291.721662\n",
      "Epoch 139 | train RMSE: 258.864567 | val RMSE: 267.660890\n",
      "Epoch 140 | train RMSE: 236.777675 | val RMSE: 245.499955\n",
      "Epoch 141 | train RMSE: 219.485867 | val RMSE: 223.823645\n",
      "Epoch 142 | train RMSE: 204.242057 | val RMSE: 206.118801\n",
      "Epoch 143 | train RMSE: 190.403020 | val RMSE: 189.273833\n",
      "Epoch 144 | train RMSE: 175.827395 | val RMSE: 171.395747\n",
      "Epoch 145 | train RMSE: 165.043501 | val RMSE: 157.191768\n",
      "Epoch 146 | train RMSE: 161.268776 | val RMSE: 148.920032\n",
      "Epoch 147 | train RMSE: 146.710593 | val RMSE: 132.259518\n",
      "Epoch 148 | train RMSE: 138.578754 | val RMSE: 121.902916\n",
      "Epoch 149 | train RMSE: 130.384839 | val RMSE: 116.608812\n",
      "Epoch 150 | train RMSE: 124.450824 | val RMSE: 103.637107\n",
      "Epoch 151 | train RMSE: 118.209113 | val RMSE: 98.707842\n",
      "Epoch 152 | train RMSE: 113.893400 | val RMSE: 91.127630\n",
      "Epoch 153 | train RMSE: 108.701241 | val RMSE: 87.236913\n",
      "Epoch 154 | train RMSE: 104.954651 | val RMSE: 75.905217\n",
      "Epoch 155 | train RMSE: 101.264929 | val RMSE: 74.180951\n",
      "Epoch 156 | train RMSE: 100.387671 | val RMSE: 77.652308\n",
      "Epoch 157 | train RMSE: 95.535967 | val RMSE: 66.923116\n",
      "Epoch 158 | train RMSE: 99.206598 | val RMSE: 67.388389\n",
      "Epoch 159 | train RMSE: 92.304471 | val RMSE: 58.827147\n",
      "Epoch 160 | train RMSE: 91.258652 | val RMSE: 55.803405\n",
      "Epoch 161 | train RMSE: 87.140949 | val RMSE: 58.330669\n",
      "Epoch 162 | train RMSE: 85.022627 | val RMSE: 53.019412\n",
      "Epoch 163 | train RMSE: 87.674152 | val RMSE: 54.195567\n",
      "Epoch 164 | train RMSE: 90.731718 | val RMSE: 45.043865\n",
      "Epoch 165 | train RMSE: 78.847995 | val RMSE: 44.332052\n",
      "Epoch 166 | train RMSE: 77.826394 | val RMSE: 48.700172\n",
      "Epoch 167 | train RMSE: 79.664404 | val RMSE: 39.855948\n",
      "Epoch 168 | train RMSE: 76.005210 | val RMSE: 39.684172\n",
      "Epoch 169 | train RMSE: 74.134018 | val RMSE: 36.560884\n",
      "Epoch 170 | train RMSE: 73.226516 | val RMSE: 33.988071\n",
      "Epoch 171 | train RMSE: 77.504050 | val RMSE: 31.773385\n",
      "Epoch 172 | train RMSE: 71.856248 | val RMSE: 43.565498\n",
      "Epoch 173 | train RMSE: 71.499836 | val RMSE: 36.852322\n",
      "Epoch 174 | train RMSE: 75.491479 | val RMSE: 30.139320\n",
      "Epoch 175 | train RMSE: 68.711359 | val RMSE: 36.931673\n",
      "Epoch 176 | train RMSE: 69.481866 | val RMSE: 28.450808\n",
      "Epoch 177 | train RMSE: 68.992487 | val RMSE: 31.296543\n",
      "Epoch 178 | train RMSE: 66.501126 | val RMSE: 35.177532\n",
      "Epoch 179 | train RMSE: 67.137953 | val RMSE: 41.521752\n",
      "Epoch 180 | train RMSE: 68.732973 | val RMSE: 30.088024\n",
      "Epoch 181 | train RMSE: 67.160218 | val RMSE: 28.816408\n",
      "Epoch 182 | train RMSE: 64.511699 | val RMSE: 28.801343\n",
      "Epoch 183 | train RMSE: 65.673072 | val RMSE: 31.928070\n",
      "Epoch 184 | train RMSE: 67.187200 | val RMSE: 23.820884\n",
      "Epoch 185 | train RMSE: 69.471779 | val RMSE: 32.337949\n",
      "Epoch 186 | train RMSE: 63.605667 | val RMSE: 39.399097\n",
      "Epoch 187 | train RMSE: 64.764733 | val RMSE: 24.522638\n",
      "Epoch 188 | train RMSE: 65.226702 | val RMSE: 29.119686\n",
      "Epoch 189 | train RMSE: 65.021782 | val RMSE: 36.097303\n",
      "Epoch 190 | train RMSE: 65.772324 | val RMSE: 35.492175\n",
      "Epoch 191 | train RMSE: 64.378933 | val RMSE: 29.803689\n",
      "Epoch 192 | train RMSE: 60.630723 | val RMSE: 34.308893\n",
      "Epoch 193 | train RMSE: 62.176981 | val RMSE: 30.699112\n",
      "Epoch 194 | train RMSE: 58.770197 | val RMSE: 30.380080\n",
      "Epoch 195 | train RMSE: 61.328780 | val RMSE: 21.960419\n",
      "Epoch 196 | train RMSE: 63.067062 | val RMSE: 31.126742\n",
      "Epoch 197 | train RMSE: 66.167431 | val RMSE: 36.929040\n",
      "Epoch 198 | train RMSE: 59.410086 | val RMSE: 26.362013\n",
      "Epoch 199 | train RMSE: 59.594220 | val RMSE: 31.136410\n",
      "Epoch 200 | train RMSE: 61.169809 | val RMSE: 21.937452\n",
      "Epoch 201 | train RMSE: 64.870660 | val RMSE: 41.834932\n",
      "Epoch 202 | train RMSE: 60.581920 | val RMSE: 23.807799\n",
      "Epoch 203 | train RMSE: 59.233665 | val RMSE: 28.749522\n",
      "Epoch 204 | train RMSE: 58.138782 | val RMSE: 20.575679\n",
      "Epoch 205 | train RMSE: 58.784481 | val RMSE: 20.575984\n",
      "Epoch 206 | train RMSE: 56.573776 | val RMSE: 32.632158\n",
      "Epoch 207 | train RMSE: 63.638939 | val RMSE: 25.139009\n",
      "Epoch 208 | train RMSE: 58.735744 | val RMSE: 22.626580\n",
      "Epoch 209 | train RMSE: 56.078982 | val RMSE: 22.219385\n",
      "Epoch 210 | train RMSE: 54.621351 | val RMSE: 23.611081\n",
      "Epoch 211 | train RMSE: 55.676265 | val RMSE: 31.084329\n",
      "Epoch 212 | train RMSE: 53.331844 | val RMSE: 15.947995\n",
      "Epoch 213 | train RMSE: 58.642447 | val RMSE: 28.628422\n",
      "Epoch 214 | train RMSE: 55.731265 | val RMSE: 32.083788\n",
      "Epoch 215 | train RMSE: 59.652114 | val RMSE: 21.661455\n",
      "Epoch 216 | train RMSE: 56.876812 | val RMSE: 27.279539\n",
      "Epoch 217 | train RMSE: 55.291053 | val RMSE: 26.002327\n",
      "Epoch 218 | train RMSE: 54.558374 | val RMSE: 17.825575\n",
      "Epoch 219 | train RMSE: 54.251452 | val RMSE: 19.549247\n",
      "Epoch 220 | train RMSE: 55.412650 | val RMSE: 22.516773\n",
      "Epoch 221 | train RMSE: 53.316483 | val RMSE: 27.144438\n",
      "Epoch 222 | train RMSE: 54.308859 | val RMSE: 14.760947\n",
      "Epoch 223 | train RMSE: 51.205736 | val RMSE: 26.506346\n",
      "Epoch 224 | train RMSE: 49.675861 | val RMSE: 31.142794\n",
      "Epoch 225 | train RMSE: 52.685628 | val RMSE: 33.234139\n",
      "Epoch 226 | train RMSE: 51.581038 | val RMSE: 23.162378\n",
      "Epoch 227 | train RMSE: 51.436988 | val RMSE: 27.345002\n",
      "Epoch 228 | train RMSE: 49.647615 | val RMSE: 31.511615\n",
      "Epoch 229 | train RMSE: 51.880160 | val RMSE: 30.656704\n",
      "Epoch 230 | train RMSE: 54.902557 | val RMSE: 31.972349\n",
      "Epoch 231 | train RMSE: 50.178818 | val RMSE: 16.523631\n",
      "Epoch 232 | train RMSE: 52.045832 | val RMSE: 14.341032\n",
      "Epoch 233 | train RMSE: 49.809885 | val RMSE: 29.596062\n",
      "Epoch 234 | train RMSE: 48.549326 | val RMSE: 31.118029\n",
      "Epoch 235 | train RMSE: 48.386446 | val RMSE: 17.281046\n",
      "Epoch 236 | train RMSE: 48.079813 | val RMSE: 16.974943\n",
      "Epoch 237 | train RMSE: 46.588325 | val RMSE: 24.650865\n",
      "Epoch 238 | train RMSE: 49.963549 | val RMSE: 30.318940\n",
      "Epoch 239 | train RMSE: 47.350992 | val RMSE: 17.181000\n",
      "Epoch 240 | train RMSE: 47.986266 | val RMSE: 26.895016\n",
      "Epoch 241 | train RMSE: 45.749846 | val RMSE: 16.592363\n",
      "Epoch 242 | train RMSE: 51.886599 | val RMSE: 33.501160\n",
      "Epoch 243 | train RMSE: 46.271292 | val RMSE: 23.440883\n",
      "Epoch 244 | train RMSE: 47.735372 | val RMSE: 43.756134\n",
      "Epoch 245 | train RMSE: 48.957577 | val RMSE: 30.227162\n",
      "Epoch 246 | train RMSE: 45.482106 | val RMSE: 14.804443\n",
      "Epoch 247 | train RMSE: 44.954971 | val RMSE: 19.402310\n",
      "Epoch 248 | train RMSE: 44.128797 | val RMSE: 29.848400\n",
      "Epoch 249 | train RMSE: 45.489123 | val RMSE: 34.306470\n",
      "Epoch 250 | train RMSE: 43.983824 | val RMSE: 23.820825\n",
      "Epoch 251 | train RMSE: 51.575446 | val RMSE: 17.946791\n",
      "Epoch 252 | train RMSE: 41.962341 | val RMSE: 17.056016\n",
      "Epoch 253 | train RMSE: 42.334198 | val RMSE: 18.145556\n",
      "Epoch 254 | train RMSE: 42.708425 | val RMSE: 18.239913\n",
      "Epoch 255 | train RMSE: 42.656307 | val RMSE: 24.442990\n",
      "Epoch 256 | train RMSE: 43.813602 | val RMSE: 24.943714\n",
      "Epoch 257 | train RMSE: 41.925918 | val RMSE: 29.092311\n",
      "Epoch 258 | train RMSE: 46.881602 | val RMSE: 22.687794\n",
      "Epoch 259 | train RMSE: 45.592899 | val RMSE: 41.599952\n",
      "Epoch 260 | train RMSE: 44.541100 | val RMSE: 25.891746\n",
      "Epoch 261 | train RMSE: 44.136873 | val RMSE: 23.724283\n",
      "Epoch 262 | train RMSE: 44.592252 | val RMSE: 38.444535\n",
      "Epoch 263 | train RMSE: 43.484924 | val RMSE: 19.026493\n",
      "Epoch 264 | train RMSE: 43.305950 | val RMSE: 29.657444\n",
      "Epoch 265 | train RMSE: 40.116267 | val RMSE: 18.983374\n",
      "Epoch 266 | train RMSE: 42.059856 | val RMSE: 14.295859\n",
      "Epoch 267 | train RMSE: 41.873473 | val RMSE: 14.158011\n",
      "Epoch 268 | train RMSE: 39.689338 | val RMSE: 32.833832\n",
      "Epoch 269 | train RMSE: 43.247325 | val RMSE: 19.492886\n",
      "Epoch 270 | train RMSE: 37.505802 | val RMSE: 17.959376\n",
      "Epoch 271 | train RMSE: 37.984352 | val RMSE: 22.184493\n",
      "Epoch 272 | train RMSE: 40.407633 | val RMSE: 22.191122\n",
      "Epoch 273 | train RMSE: 38.476654 | val RMSE: 18.548335\n",
      "Epoch 274 | train RMSE: 38.978128 | val RMSE: 23.318761\n",
      "Epoch 275 | train RMSE: 41.970985 | val RMSE: 16.537281\n",
      "Epoch 276 | train RMSE: 40.856780 | val RMSE: 21.862868\n",
      "Epoch 277 | train RMSE: 38.859497 | val RMSE: 14.369644\n",
      "Epoch 278 | train RMSE: 38.617402 | val RMSE: 29.920155\n",
      "Epoch 279 | train RMSE: 38.846973 | val RMSE: 20.868821\n",
      "Epoch 280 | train RMSE: 37.473248 | val RMSE: 17.536210\n",
      "Epoch 281 | train RMSE: 37.603392 | val RMSE: 18.624564\n",
      "Epoch 282 | train RMSE: 48.628015 | val RMSE: 15.537451\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m model.train()\n\u001b[32m      3\u001b[39m train_se_sum, train_n = \u001b[32m0.0\u001b[39m, \u001b[32m0.0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# (B, Lmax, 1)\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ds-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ds-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ds-env/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mcollate_pad\u001b[39m\u001b[34m(batch, pad_value)\u001b[39m\n\u001b[32m     14\u001b[39m xs, ys = \u001b[38;5;28mzip\u001b[39m(*batch)\n\u001b[32m     15\u001b[39m lengths = torch.tensor([t.size(\u001b[32m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m xs], dtype=torch.long)  \u001b[38;5;66;03m# (B,)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m X = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_value\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, Lmax, F)\u001b[39;00m\n\u001b[32m     19\u001b[39m Y = torch.nn.utils.rnn.pad_sequence(ys, batch_first=\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value=\u001b[32m0.0\u001b[39m)  \u001b[38;5;66;03m# (B, Lmax, 1)\u001b[39;00m\n\u001b[32m     20\u001b[39m B, Lmax, _ = X.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ds-env/lib/python3.12/site-packages/torch/nn/utils/rnn.py:475\u001b[39m, in \u001b[36mpad_sequence\u001b[39m\u001b[34m(sequences, batch_first, padding_value, padding_side)\u001b[39m\n\u001b[32m    471\u001b[39m         sequences = sequences.unbind(\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m    473\u001b[39m \u001b[38;5;66;03m# assuming trailing dimensions and type of all the Tensors\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[38;5;66;03m# in sequences are same and fetching those from sequences[0]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m    \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    477\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    train_se_sum, train_n = 0.0, 0.0\n",
    "\n",
    "    for X, Y, pad_mask, loss_mask, lengths in train_loader:\n",
    "        X, Y, pad_mask, loss_mask = (\n",
    "            X.to(device),\n",
    "            Y.to(device),\n",
    "            pad_mask.to(device),\n",
    "            loss_mask.to(device),\n",
    "        )\n",
    "        pred = model(X, src_key_padding_mask=pad_mask)  # (B, Lmax, 1)\n",
    "\n",
    "        mse_map = loss_fn(pred, Y)  # (B, Lmax, 1)\n",
    "        loss = (mse_map * loss_mask).sum() / loss_mask.sum().clamp_min(1)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_se_sum += (mse_map * loss_mask).sum().item()\n",
    "        train_n += loss_mask.sum().item()\n",
    "\n",
    "    train_mse = train_se_sum / max(train_n, 1.0)\n",
    "\n",
    "    # Валидация\n",
    "    model.eval()\n",
    "    val_se_sum, val_n = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for X, Y, pad_mask, loss_mask, lengths in val_loader:\n",
    "            X, Y, pad_mask, loss_mask = (\n",
    "                X.to(device),\n",
    "                Y.to(device),\n",
    "                pad_mask.to(device),\n",
    "                loss_mask.to(device),\n",
    "            )\n",
    "            pred = model(X, src_key_padding_mask=pad_mask)\n",
    "            mse_map = loss_fn(pred, Y)\n",
    "            val_se_sum += (mse_map * loss_mask).sum().item()\n",
    "            val_n += loss_mask.sum().item()\n",
    "\n",
    "    val_mse = val_se_sum / max(val_n, 1.0)\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | train RMSE: {math.sqrt(train_mse):.6f} | val RMSE: {math.sqrt(val_mse):.6f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
